---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# Introduction

This notebook identifies and extracts the spreadsheet data published by NHS Digital for a range of topics. It makes use of 

NHS Digital publishes a range of public health data in the form of spreadsheets



```{r, include = FALSE}

knitr::opts_chunk$set(cache = TRUE, warning = FALSE)

if(!require("myScrapers"))devtools::install_github("julianflowers/myScrapers")
library(myScrapers)
library(rvest)
library(tidyverse, quietly = TRUE)

```

Identifying relevant links on the new NHSD website is an iterative process.

Define functions to extract URLs, create links and find .csv files

```{r identify url}



get_nhsd_urls <- function(url){
  
  require(rvest)
  require(tidyverse)
  
  read_html(url) %>%
    html_nodes("a") %>%
    html_attr("href") %>%
    .[grepl("statistical", .)]
}  
  
create_nhsd_urls <- function(links){
  nhsd_url <- paste0("https://digital.nhs.uk", links)
}

find_nhsd_csv <- function(link){
  csv <- link %>%
  read_html() %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  .[grepl("csv|xlsx", .)]
  
  csv
}

get_nhsd_titles <- function(link){
  
  csv_title <- link %>%
  read_html() %>%
  html_nodes(".attachment") %>%
  html_text() %>%
  .[grepl("CSV|Data File", .)] %>%
  tm::stripWhitespace()
  
  csv_title
}
```

## Identify root pages for statistical publications

```{r}

url <- "https://digital.nhs.uk/data-and-information/publications/statistical"
urls <- get_nhsd_urls(url)
urls

```


## Create links

```{r}

links <- create_nhsd_urls(urls)
links



```

## Find individual pages

```{r}
## for 1 set - eg trends in consultation rates

links1 <- map(links, function(x) get_nhsd_urls(x) )

links1[[1]][1:6]
```

We now have a list of all the subpages for each topic.

We can now try and identify all the relevant csv or xlsx files on each page.

## Find csv/xlsx

```{r}
links2 <- map(links1, function(x) create_nhsd_urls(x))

links2 <- flatten(links2)



csv <- map(links2, function(x) find_nhsd_csv(x))

titles <- map(links2, function(x) get_nhsd_titles(x))

names(csv) <- titles

csv_flat <- flatten(csv)

```

This produces a list of `r length(csv)` sets of csv/xlsx files we can download. There are `r length(csv_flat)` files in all.

We can in read in csv files directly from the link (xlsx files need to be downloaded first).

```{r}
csvs <- csv_flat %>% .[grepl("csv", .)] 


downloads <- map(csvs[c(1:4)], read_csv, na = "*")

glimpse(downloads)
```

We can see some cleaning will be needed to remove excess columns, convert character fields to numeric an so on.


