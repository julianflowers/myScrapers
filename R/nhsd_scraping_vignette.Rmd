---
title: "Identifying and extracting spreadsheet data from NHS Digital"
author: "Julian Flowers"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{phe blog extraction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

This notebook identifies and extracts the spreadsheet data published by NHS Digital for a range of topics. It makes use of functions in the `myScrapers` package which allow us to iteratively parse the NHS Digital website to identify the spreadsheets and csv files attached to statistical publications - "https://digital.nhs.uk/data-and-information/publications/statistical"


It makes use of 4 functions:

* `get_nhsd_url` which finds urls
* `create_nhsd_url` which create urls 
* `get_nhsd_csv` which finds spreadsheet links
* `get_nhsd_titles` which finds resource titles



```{r, include = FALSE}

knitr::opts_chunk$set(cache = TRUE, warning = FALSE)

if(!require("myScrapers"))devtools::install_github("julianflowers/myScrapers")
library(pacman)
p_load(myScrapers, tidyverse)

```

Identifying relevant links on the new NHSD website is an iterative process.

Define functions to extract URLs, create links and find .csv files


## Identify root pages for statistical publications

```{r urls}

url <- "https://digital.nhs.uk/data-and-information/publications/statistical"
urls <- get_nhsd_urls(url)
urls

```


## Create links

```{r}

links <- create_nhsd_urls(urls)
links



```

## Find individual pages

```{r}
## for 1 set - eg trends in consultation rates

links1 <- map(links, function(x) get_nhsd_urls(x) )

links1[[1]][1:6]
```

We now have a list of all the subpages for each topic.

We can now try and identify all the relevant csv or xlsx files on each page. We can use the `find_nhsd_csv` function for this, and `find_nhsd_titles`  for metadata.

## Find csv/xlsx

```{r}
links2 <- map(links1, function(x) create_nhsd_urls(x))

links2 <- flatten(links2)

csv <- map(links2, function(x) find_nhsd_csv(x))

titles <- map(links2, function(x) get_nhsd_titles(x))

names(csv) <- titles

csv_flat <- flatten(csv)

```

This produces a list of `r length(csv)` sets of csv/xlsx files we can download. There are `r length(csv_flat)` files in all.

We can in read in csv files directly from the link (xlsx files need to be downloaded first).

```{r}
csvs <- csv_flat %>% .[grepl("csv", .)] 


downloads <- map(csvs[c(29)], read_csv, na = "*")

head(downloads)
```

We can see some cleaning will be needed to remove excess columns, convert character fields to numeric an so on.


