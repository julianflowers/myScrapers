---
title: "Web scraping"
author: "Julian Flowers"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE, 
  message = FALSE, 
  cache = TRUE, 
  echo = TRUE, 
  comment = "#>"
)

library(tidyverse)
library(myScrapers)
library(Rcrawler)

```

This vignette illustrates some of the basics of web-scraping and some features of the `myScrapers` package - in particular simple web-scraping functions. We also show some functions in the package specifically designed to retrieve public health information for public health practitioners.

## Web scraping basics in R

The basic toolkit is:

* a Url (link) you want to obtain data from
* the `rvest` package in R or `beautiful soup` in Python
* the [selector gadget](https://selectorgadget.com/) extension for web browsers

A basic knowledge of html is helpful - especially elements and tags. For example paragraphs are defined with the `<p>` tag, links with the `<a>` tag and so on. 
("https://www.w3schools.com/html/html_intro.asp")`

Selectorgadget enables finding tags used in style sheets if page layout is more sophisticated.

## Installing the package

The package is only available on Github and can be downloaded using `devtools`.

```{r, eval=FALSE}

library(devtools)
devtools::install_github("julianflowers/myScrapers")

```

## Simple web-scraping

Web scraping is a set of techniques to obtain information or data from websites. In R the `rvest` and `httr` packages are the mainstay of scraping (there is also `Rcrawler`). These import and read html and xml pages into R which can then be parsed and analysed.

In Python `beautiful soup` is the main package for web scraping.

Both are built on top of web commands which underpin the exchange of information across the internet. 

In R, given any weblink or url we can read the site with the `GET` function in `httr`.

Lets use https:\\fingertips.phe.org.uk...

```{r read fingertips}

url <- "https://fingertips.phe.org.uk"

html <- GET(url)

html

```

```{r using rvest}

rvest <- read_html(url)
rvest

rvest %>%
  html_nodes("a") %>%
  .[1] %>%
  html_attr("href")
```



In `myScrapers` there are 4 primary functions built on these:

* `get_page_links` which identifies the links on a webpage
* `get_page_text` which extracts text from a webpage
* `get_page_docs` which identifies pdf or doc files on a page
* `get_page_csv` which finds csv files and spreadsheets on a page

## Examples

We can use `get_page_links`  to extract information from following page of PHE statistical releases. https://www.gov.uk/government/statistics?departments%5B%5D=public-health-england

```{r}



url <- "https://www.gov.uk/government/statistics?departments%5B%5D=public-health-england"

get_page_links(url) %>%
  .[19:40]

```

## Use cases

We'll use [GP in hours syndromic surveillance](https://www.gov.uk/government/publications/gp-in-hours-weekly-bulletins-for-2018) data to illustrate further uses. This report "Monitors the number of people who visit their GP during surgery hours under the syndromic surveillance system."

The system publishes weekly reports and spreadsheets  - to obtain a year's worth of these reports manually would require 104 separate downloads.

Using a webscraping approach this can be achieved in a few lines of code.

### Identifying reports

The code below identifies all the pdf reports on the page.

```{r}
urls <- "https://www.gov.uk/government/publications/gp-in-hours-weekly-bulletins-for-2018"

get_page_links(urls) %>%
  .[grepl("pdf$", .)] %>%
  head(10) %>%
  unique()

```

We can then use the `downloader` package to download the pdfs:

```{r, results="hide"}
## not run
library(downloader)

(get_page_links(urls) %>%
  .[grepl("pdf$", .)] %>%
  head(10) %>%
  unique() %>%
  purrr::map(., ~download(.x, destfile = basename(.x))))
  



```


## Identifying data (spreadsheet links)

We can take a similar approach to spreadsheets.

```{r, echo = FALSE, results="hide"}
urls <- "https://www.gov.uk/government/publications/gp-in-hours-weekly-bulletins-for-2018"

get_page_links(urls) %>%
  .[grepl("xls.?$", .)] %>%
  head(10) %>%
  unique() %>%
  purrr::map(., ~downloader::download(.x, destfile = basename(.x)))
  

```

Having downloaded the reports or spreadsheets it is now straightforward to import them for further analysis.

```{r}

library(readxl)
files <- list.files(pattern = ".xls")

data <- purrr::map(files, ~(read_excel(.x,  sheet = "Local Authority", na = "*", 
    skip = 4)))

head(data)



```

## Scraping text

This follows the same principle. The function `get_page_text` is designed to extract text from a webpage. For example, we can extract the text from an article about Matt Hancock's description of "predictive prevention'.

> `r get_page_text("https://www.thetimes.co.uk/article/nhs-will-use-phone-data-to-predict-threats-to-your-health-r7085zqfq") %>% .[1:4]`


## Analysing Duncan Selbie's Friday messages

Using simple functions it is relatively easy to scrape Duncan Selbie's blogs into a data frame for further analysis.

The base URL is https://publichealthmatters.blog.gov.uk/category/duncan-selbie-friday-message/, and there are 8 pages of results so the first task is to create a list of urls.

```{r}

url_ds <- "https://publichealthmatters.blog.gov.uk/category/duncan-selbie-friday-message/"
url_ds1 <- paste0(url_ds, "page/", 2:8)
urls_ds <- c(url_ds, url_ds1)

```

Then we can extract links and isolate those specific to the friday messages

```{r}

links <- purrr::map(urls_ds, ~(get_page_links(.x))) 

friday_message <- links %>% purrr::flatten() %>%.[grepl("duncan-selbies-friday-message", .)] %>% .[!grepl("comments", .)] %>% unique()

head(friday_message)

```

and then extract blog text:

```{r}
library(tm)
library(magrittr)

blog_text <- purrr::map(friday_message, ~(get_page_text(.x)))
blog_text <- purrr::map(blog_text, ~(str_remove(.x, "\\n")))
blog_text <- purrr::map(blog_text, ~(str_remove(.x, "    GOV.UK blogs use cookies to make the site simpler. Find out more about cookies\n  ")))
blog_text <- purrr::map(blog_text, ~(str_remove(.x, "Dear everyone")))

blog_title <- purrr::map(blog_text, 2)
names(blog_text) <- blog_title

blog_text1 <- purrr::map(blog_text, extract, 5:11)
blog_text2 <- purrr::map(blog_text1, data.frame)
blog_text2 <- purrr::map_df(blog_text2, bind_rows)
blog_text2 <- blog_text2 %>% mutate(text = clean_texts(.x..i..))

```


We can then visualise with, for example, a wordcloud.

```{r fig.height=8, fig.width=8}
library(quanteda)

corp <- corpus(blog_text2$text)
dfm <- dfm(corp, ngrams = 2, remove = c("government_licence", "open_government", "public_health", "official_blog", "blog_public", "health_england", "cancel_reply", "content available", "health_blog", "licence_v", "best_wishes", "otherwise_stated", "except_otherwise", "friday_messages", "best_wishes", 
                                        "available_open"))

textplot_wordcloud(dfm, color = viridis::plasma(n = 10))

```


## Additional functions

I have added a few functions to the package.

`get_dsph_england` returns a list of local authorities and their current DsPH. It scrapes https://www.gov.uk/government/publications/directors-of-public-health-in-england--2/directors-of-public-health-in-england

```{r}

dsph <- get_dsph_england()
dsph %>%
  knitr::kable()

```

`get_phe_catalogue` identifies all the PHE publications on GOV.UK. For this function you have to set the n = argument. We recommend starting at n = 110. This produces an interactive searchable table of links. 

```{r, cache=TRUE}

cat <- get_phe_catalogue(n=110)

cat

```


## Reviewing parliamentary questions (PQs)

We have added a `get_pq` function built on the `hansard` package to extract PQs addressed to,  answered by or mentioning PHE. This takes a start date as an argument in the form yyyy-mm-dd.

```{r pqs}


pqs <- get_pqs(start_date = "2019-03-01")

pqs


```

We can look at the categories of questions asked.


```{r}

pqs %>%
  group_by(hansard_category) %>%
  count() %>%
  arrange(-n) %>%
  top_n(10)

```

## Using `myScrapers` to extract NICE guidance

We can use the toolkit to extract NICE Public Health Guidance as follows:

* Firstly we'll identify the URLs for NICE PH guidance - they related to https://www.nice.org.uk/guidance/published?type=ph

* Then create a full URL for recommendations

* Then extract the text



```{r}


url <- "https://www.nice.org.uk/guidance/published?type=ph"

links <- get_page_links(url)[13:22] ##first 10  sets of guidance
links1 <- purrr::map(links, ~(paste0("https://www.nice.org.uk", .x, "/chapter/Recommendations")))

pander::panderOptions("table.style", "multiline")
pander::panderOptions("table.alignment.default", "left")
recommendations <- purrr::map(links1, ~(get_page_text(.x))) %>% purrr::map(., data.frame) 
head(recommendations, 1) %>% 
  knitr::kable()

```


